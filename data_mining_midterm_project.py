# -*- coding: utf-8 -*-
"""Data Mining Midterm Project (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b6EsomqVfm5xqySMiGGdd5nFp7spJVUP

# Data Mining CS-634_Midterm_Project

# Importing_Libraries
"""

import numpy as np
import pandas as pd
import itertools
from itertools import combinations
import warnings
warnings.filterwarnings("ignore")
import time
import os


"""# Importing Datasets"""

import os

def select_dataset():
    # Get list of files in the folder
    files = os.listdir("Datasets")

    # Print the list of datasets
    print("Available datasets:")
    for i, file in enumerate(files):
        print(f"{i+1}. {file}")

    # Ask user to select a dataset
    choice = int(input("Enter the number corresponding to the dataset you want to select: "))

    # Validate user input
    if 1 <= choice <= len(files):
        selected_dataset = files[choice-1]
        print(f"You selected '{selected_dataset}' as the dataset.")
        return selected_dataset
    else:
        print("Invalid choice. Please select a number within the range.")
        return None

data = select_dataset()
if data:
    # Now you can use 'data' variable to further process the selected dataset
    print(f"Processing '{data}'...")
    # Add your code to process the selected dataset here


"""# Loading_Data"""

df = pd.read_csv("Datasets//" + data)

print(df.head())

"""# Assigining an index to each item listed in"""
item_list = list(df.columns)
item_dict = dict()

for i, item in enumerate(item_list):
    item_dict[item] = i + 1

item_dict

"""# Extracting Transactions from the Data"""

transactions = list()

for i, row in df.iterrows():
    transaction = set()

    for item in item_dict:
        if row[item] == 't':
            transaction.add(item_dict[item])
    transactions.append(transaction)

transactions

min_support = int(input("Enter Minimum Support : ")) #Enter minimum support
min_support = (min_support)/100


confidence = int(input("Enter the Minimum Confidence : ")) #Enter minimun confidence
min_confidence = (confidence)/100


start = time.time()
"""# Get Support Function that evaluates the support value for a set given all the transactions"""

def get_support(transactions, item_set):
    match_count = 0
    for transaction in transactions:
        if item_set.issubset(transaction):
            match_count += 1

    return float(match_count/len(transactions))

"""# self_join performs join based on the last level valid sets. It joins each sets together by performing union and if the length exceeds the current level, it will skip that set."""

def self_join(frequent_item_sets_per_level, level):
    current_level_candidates = list()
    last_level_items = frequent_item_sets_per_level[level - 1]

    if len(last_level_items) == 0:
        return current_level_candidates

    for i in range(len(last_level_items)):
        for j in range(i+1, len(last_level_items)):
            itemset_i = last_level_items[i][0]
            itemset_j = last_level_items[j][0]
            union_set = itemset_i.union(itemset_j)

            if union_set not in current_level_candidates and len(union_set) == level:
                current_level_candidates.append(union_set)

    return current_level_candidates

def get_single_drop_subsets(item_set):
    single_drop_subsets = list()
    for item in item_set:
        temp = item_set.copy()
        temp.remove(item)
        single_drop_subsets.append(temp)

    return single_drop_subsets

def is_valid_set(item_set, prev_level_sets):
    single_drop_subsets = get_single_drop_subsets(item_set)

    for single_drop_set in single_drop_subsets:
        if single_drop_set not in prev_level_sets:
            return False
    return True

def pruning(frequent_item_sets_per_level, level, candidate_set):
    post_pruning_set = list()
    if len(candidate_set) == 0:
        return post_pruning_set

    prev_level_sets = list()
    for item_set, _ in frequent_item_sets_per_level[level - 1]:
        prev_level_sets.append(item_set)

    for item_set in candidate_set:
        if is_valid_set(item_set, prev_level_sets):
            post_pruning_set.append(item_set)

    return post_pruning_set

"""# This is the main function which uses all the above described Utility functions to implement the Apriori Algorithm and generate the list of frequent itemsets for each level for the provided transactions and min_support value."""

from collections import defaultdict
def apriori(min_support):
    frequent_item_sets_per_level = defaultdict(list)
    print("level : 1", end = " ")

    for item in range(1, len(item_list) + 1):
        support = get_support(transactions, {item})
        if support >= min_support:
            frequent_item_sets_per_level[1].append(({item}, support))

    for level in range(2, len(item_list) + 1):
        print(level, end = " ")
        current_level_candidates = self_join(frequent_item_sets_per_level, level)

        post_pruning_candidates = pruning(frequent_item_sets_per_level, level, current_level_candidates)
        if len(post_pruning_candidates) == 0:
            break

        for item_set in post_pruning_candidates:
            support = get_support(transactions, item_set)
            if support >= min_support:
                frequent_item_sets_per_level[level].append((item_set, support))

    return frequent_item_sets_per_level



frequent_item_sets_per_level = apriori(min_support)

"""# The below code produces a dictionary called item_support_dict which from frequent_item_sets_per_level that maps items to their support values"""

item_support_dict = dict()
item_list = list()

key_list = list(item_dict.keys())
val_list = list(item_dict.values())

for level in frequent_item_sets_per_level:
    for set_support_pair in frequent_item_sets_per_level[level]:
        for i in set_support_pair[0]:
            item_list.append(key_list[val_list.index(i)])
        item_support_dict[frozenset(item_list)] = set_support_pair[1]
        item_list = list()

"""# The find_subset function takes the item and item_length as parameter and it returns all the possible combinations of elements inside the items"""

def find_subset(item, item_length):
    combs = []
    for i in range(1, item_length + 1):
        combs.append(list(combinations(item, i)))

    subsets = []
    for comb in combs:
        for elt in comb:
            subsets.append(elt)

    return subsets

"""# This function generates the association rules in accordance withe the minimum confidence value and the provided dictionary of itemsets against their support values. It takes the mininmum confidence value and support_dict as a parameter, and returns rules as a list."""

def association_rules(min_confidence, support_dict):
    rules = list()
    for item, support in support_dict.items():
        item_length = len(item)

        if item_length > 1:
            subsets = find_subset(item, item_length)

            for A in subsets:
                B = item.difference(A)

                if B:
                    A = frozenset(A)

                    AB = A | B

                    confidence = support_dict[AB] / support_dict[A]
                    if confidence >= min_confidence:
                        rules.append((A, B, confidence))

    return rules


association_rules = association_rules(min_confidence, support_dict = item_support_dict)



"""# Output"""
print("Brute Force Results : \n")
print("Number of rules: ", len(association_rules))

for rule in association_rules:
    print('{0} -> {1} <confidence: {2}>'.format(set(rule[0]), set(rule[1]), rule[2]))

end = time.time()

print("\n\nTime taken for Brute Force Approach : ", end - start, "seconds \n\n")

"""# Using the_mlxtend Librarry and implementing the the apriori algorithm"""

from mlxtend.frequent_patterns import apriori, fpgrowth
from mlxtend.frequent_patterns import association_rules

# Convert the 'Items' column to a one-hot encoded format
df_encoded = df.replace('t', 1)

start_apriori = time.time()
# Apply Apriori algorithm
frequent_itemsets = apriori(df_encoded.fillna(0), min_support=min_support, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)


print("\nApriori results \n\n")
# Display the frequent itemsets and association rules
print("Frequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents']])
end_apriori = time.time()

print("\n\n Time taken for apriori : ", end_apriori - start_apriori , "seconds \n\n")

start_FPgrowth = time.time()
# Apply FPgrowth algorithm
frequent_itemsets = fpgrowth(df_encoded.fillna(0), min_support=min_support, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)


print("\nresults \n\n")
# Display the frequent itemsets and association rules
print("Frequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents']])
end_FPgrowth = time.time()

print("\n\n Time taken for FPgrowth : ", end_FPgrowth - start_FPgrowth , "seconds \n\n")